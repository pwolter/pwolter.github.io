[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Thank you for being here. I hope you enjoyed what you read!\nI am a software engineer interested in big data and analytics as a whole. I like data science, machine learning and all in between.\nOn my spare time I scuba dive, both recreational and technically. I play ice hockey, run, bike, and snowboard. When seated still, I like to read, listen music and learn new skills.\nI am from Chile, but moved around due different reasons. Some are intolerance and some my own choices. I live in Seattle, Washington in the United States. I like it here."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The dark Side of the Spoon",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nChurn prediction using Spark\n\n\n\n\n\n\n\nSpark\n\n\nML\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2020\n\n\nPablo Wolter\n\n\n\n\n\n\n  \n\n\n\n\nDisaster Response Pipeline\n\n\n\n\n\n\n\nData Science\n\n\nAnalytics\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2020\n\n\nPablo Wolter\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing a decade of NHL Hockey\n\n\n\n\n\n\n\nData Science\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2020\n\n\nPablo Wolter\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Disaster_Response_Pipeline/index.html",
    "href": "posts/Disaster_Response_Pipeline/index.html",
    "title": "Disaster Response Pipeline",
    "section": "",
    "text": "In this project, we analyzed disaster data provided by Figure Eight to build a Natural Language Processing (NLP) model for an API that classifies disaster messages.\nThe provided data set contains real messages that were sent during disaster events. We have created a machine learning pipeline to categorize these events so that users of the application can send the messages to an appropriate disaster relief agency to take proper and timely action.\nThe project includes a web application were an emergency worker can input a new message and get classification results in several categories. The web application also displays visualizations of the data. The code can be accessed here\n\n\nWe have used the following libraries besides the regular Python Data Science stack:\n\nScikit learn\nNLTK\nsqlalchemy\nFlask\npickle 0.20\nPlotly\nargparser\n\nTo install them just run:\npip install -u scikit-learn nltk sqlalchemy flask pickleshare plotly\n\n\n\nThe project is divided into three main components:\n\n\nA Python script, process_data.py, contains the data cleaning pipeline that:\n\nLoads the messages and categories datasets\nMerges the two datasets\nCleans the data\nStores it in an SQLite database\n\n\n\n\nA Python script, train_classifier.py, that holds the code of the machine learning pipeline that:\n\nLoads data from the SQLite database saved before\nSplits the dataset into training and test sets\nBuilds a text processing and machine learning pipeline\nTrains and tunes a model using GridSearchCV\nOutputs results on the test set\nExports the final model as a pickle file\n\n\n\n\nHolds the code of the Web Interface from which the end-user will enter the message’s text to get the predictions. This page will guide the emergency worker providing predictions about the type of emergency at hand.\n\n\n\n\nThe list of the files used in this project are:\n- app\n|  - template\n|  |- master.html   # main page of web app\n|  |- go.html       # classification result page of web app\n|- run.py           # Flask file that runs the app\n\n- data\n|- disaster_categories.csv  # data to process\n|- disaster_messages.csv    # data to process\n|- process_data.py          # process data and saves it into a database\n|- DisasterResponse.db      # database to save clean data to\n\n- models\n|- train_classifier.py  # machine learning modeling and prediction\n|- classifier.pkl       # saved model\n\n- images\n|- English_No-English.png     # English/No-English messages distribution\n|- Message_distribution.png   # Messages distribution histogram\n\n- README.md\n\n\n\n\n\nRun the following commands in the project’s root directory to set up your database and model.\nTo run ETL pipeline that loads the source CSV files, cleans them and stores the cleaned resulting Pandas Dataframe in a database. Run the following under the data directory.\npython process_data.py disaster_messages.csv disaster_categories.csv DisasterResponse.db\nTo run ML pipeline that trains classifier and saves the model in a pickle file run the following from the models directory:\npython train_classifier.py ../data/DisasterResponse.db classifier.pkl\nRun the following command in the app directory to run the web application in order to do predictions:\npython run.py\n\nThen go to http://0.0.0.0:3001/ to launch the web interface of the application\nIn the box on the top enter the message to classify and click on the “Classify Message” button."
  },
  {
    "objectID": "posts/decade_of_hockey/index.html",
    "href": "posts/decade_of_hockey/index.html",
    "title": "Analyzing a decade of NHL Hockey",
    "section": "",
    "text": "Seasons from 2010 to 2019\n\n\n\nPhoto by Seth Hoffman on Unsplash\n\n\nThe first time I laced a pair of skates I was just 6 years old. My younger brother was just 4. At age 9 I started training in figure skating. One day I was trying to do a very simple jump I was struggling with following the circle on the right of the rink when the hockey team arrived. I don’t know how to explain it. The sound of their skates in the rink when they stopped just blew my mind. That sound of wheels against the tile. The delicate sound of the ball hitting the stick when passing it. That day I was sold. After my practice, I spoke with my coach and I switched that same day to hockey. The President of the club I played all my amateur career gave me her son’s stick, a small one for junior players. I never figured skated in my life again. Since that day I was a hockey player and was making that beautiful sound myself. This wasn’t ice hockey. This was roller blade hockey, 4 wheels in parallel.\n\n\nToday I don’t play it anymore and I never had the chance to play ice hockey. With this work, I am combining my two passions: data science and hockey.\nThe principal motivation of this work is to start looking at some answers to questions I have regarding the last decade’s hockey results. I am going to answer the following questions based on the data I found in Kaggle.\n\nWhat are the league’s best goalies?\nWhat are the league’s best scorer players?\nWhat are the teams that score more goals?\nWhat are the players that have most penalty minutes per team?\nWhat is the most prolific country producing NHL players?\n\nLet’s start our analysis. The notebook with the details and data cleaning and merging of the several CSV files can be found on my GitHub repository.\nThe “Country that produces more NHL players” graph source is here. The graph was uploaded to Tableau public for easy access.\n\n\n\nI started with the goalies because I found a very important issue with the save percentage score once I got to them in my work. If we graph the top 10 goalies based on save percentage score in descending order we get the following graph:\n\n\n\nFigure 1.1: Average Safe Percentage by Goalie\n\n\nWhat happened here? None of them are the goalies we are so used to see every game night and all have 100% average save percentages! The answer is these are mostly “emergency goalies”. Do you remember Scott Foster who was Blackhawk’s famous contracted emergency goalie? He never suited up for an official game during his tenure with the team. Until that lucky night during the 2017–2018 season when all Blackhawks’ goalies were injured and he was called out to (finally) dress up. He played “the longest 14 minutes of Hockey” of his life as he described it himself in one of the countless interviews he did afterward. He played ~14 minutes and did not receive a goal, so his save percentage is 100% (for his entire NHL career!). To take care of this I considered two alternatives:\n\n\nThis alternative is not fair of seasoned goalies and players in general. Veteran goalie players enter the rink to replace the starting goalie when have already received 3–4 goals (or more). So they deserve the good 100% save percentage on these circumstances.\n\n\n\nThis approach also has some caveats as this arises the question of what does make the minimum number of seasons correct? In spite of this concern, I have selected this option. I decided to set a minimum of 5 seasons with 81 games per season (41 home ice plus 41 away games).\n\n\n\nFigure 1.2: Top 10 Goalies Avg. Save Percentage\n\n\nNow we see the usual suspects. Tuukka Rask being the goalie with better avg. save percentage followed by two of my favorites: Carey Price and Henrik Lundqvist. As you can see here their numbers are very close to each other. It is thought to be an NHL goalie! The expectations are very high.\n\n\n\n\nLet’s start looking at the best players in terms of total goals.\n\n\n\nFigure 1.3: Total goals by player\n\n\nThis does not really tell us the whole story. This is not 100% accurate or fair as there are players that play fewer games than the others but do score more goals per game and/or per time on ice (minutes played per game). Let’s look at the same metric but now taking into account games played.\n\n\n\nFigure 1.4: Top 10 Players Goals Per Game\n\n\nThe goals per game tell us that Alex Ovechkin maintains the first place as he plays a lot of games and scores a lot too! He scores almost a goal every other game. Steven Stamkos follows with a very similar number but has played more than 100 fewer games compared with Ovechkin. Sidney Crosby and Evgeni Malkin had jumped to the top making the Penguins a very dangerous team! No surprise almost all are captains of their respective teams and forwards/wings. Tyler Seguin from the Dallas Stars has played significantly fewer games, just 489 but hold the 5th position, so he’s very effective scoring goals. On the other hand, Patrick Kane has the second high position in games played (on this list) but very low goals, positioning him in the second to last spot.\nLet’s look at the players that spend more time on the ice now to see if this relates to the number of goals.\n\n\n\nFigure 1.5: Top 10 Defense Avg. Time On Ice\n\n\nSorting the list by time on ice (TOI) the top 10 are all defensemen. The average for defense is around 14–25 minutes per game. That is almost half the game! Ryan Sutter plays lots of minutes despite not being present in a lot of games. Drew Doughty form the LA Kings has been playing almost half a game consistently during this decade.\nFiltering out the defensive players out we see the forwards average TOI.\n\n\n\nFigure 1.6: Top 10 Players (no Defense) Avg. Time On Ice\n\n\nForwards play less time on the ice as they tend to skate more. Anze Kopitar, Ryan Getzlaf and Sidney Crosby top the chart. Alex Ovechkin doesn’t top this chart but is a very effective player as his goals per game mark.\n\n\n\nI have averaged the Top 3 scorers per team goals to put the scoring power per team into perspective. The following graph shows the team with more scoring power thanks to its three top-scoring players.\n\n\n\nFigure 1.7: Average of total goals of the top 3 scorers per team\n\n\nHere we can see the Sharks top 3 players scored more goals in the last decade that the rest of the teams. They are followed by the Blackhawks and Penguins in second and third place respectively as the most dangerous teams. The Capitals, however, are just in position #4 despite Alex Ovechkin’s tremendous production.\nSurprisingly, when looking at the goals per game by team the Stars are the team that scores more goals per game (1 goal every 3 games guaranteed!). They are followed by the Lightning, Panthers and Oilers. The Penguins are in the fifth position a little short of scoring 1 goal every three games.\n\n\n\nFigure 1.8: Average Goals per game by Team\n\n\n\n\n\nSurprisingly these are no defense. They are forwards. Senators Chris Neil tops the chart sitting in the penalty box 10 times per 6 games!. He is followed by Cody McLeod and Tom Wilson each sitting in the penalty box once every game. All very well known for their strength and physicality.\n\n\n\nFigure 1.9: Top 10 players Penalty Minutes Per Game\n\n\n\n\n\nIt should not be any surprise to find out that Canada provides more players to the NHL than any other country.\n\n\n\nFigure 1.10: Country that produces more NHL players from 2010–2019\n\n\nThe number of Canadian players in the last decade total 1,149. Followed by the USA with 651 and then Sweden with 184. Canadian players almost double the number of American players which in turn almost quadruples the number of players from Sweden.\n\n\n\nFigure 1.11: Players per country\n\n\nThe rest of the top 10 countries only totals 380 players.\n\n\n\nThis is just a descriptive analysis of a decade of NHL hockey. During the duration of this work, a lot more questions arose that I left out from this post to be able to finish it on time. I am planning to follow up this post with a second part where I will try to answer other interesting questions. Things that will come in the future post(s) are, for example:\n\nHow do teams combine penalty minutes relate to the team losing the game?\nDoes it have any relation to the team playing in home ice or away?\nDo top scorers make a difference in the last period (3rd period) or Overtime (OT) in the games?\n\nDid your favorite player show up in the ranking presented in this work? Let me know in the comments. Also, let me know what else will you be interested in seeing analyzed in the second part. Thank you for reading it!"
  },
  {
    "objectID": "posts/sparkify/index.html",
    "href": "posts/sparkify/index.html",
    "title": "Churn prediction using Spark",
    "section": "",
    "text": "Photo by Zarak Khan on Unsplash"
  },
  {
    "objectID": "posts/sparkify/index.html#project-description",
    "href": "posts/sparkify/index.html#project-description",
    "title": "Churn prediction using Spark",
    "section": "Project description",
    "text": "Project description\nThe following project tries to predict user churn rate in a fictitious music streaming service called Sparkify.\nI used Spark in Amazon Web Services (AWS) with an Elastic Map Reduce (EMR) cluster of 3 m5.xlarge machines. One driver and two workers. The dataset size is 12 Gbytes and was read from an AWS Simple Storage Service (S3) bucket in JSON format. This file contains activity registered from users as they used the service daily.\nAs software libraries go I have used: PySpark, Python’s Spark API. AWS EMR version 5.29.0. Logistic Regression, Random Forest, Gradient Boost Trees (GBT) Classifier, and Naive Bayes form Spark’s Machine Learning Library. Pandas and Matplotlib from the standard data science Python stack."
  },
  {
    "objectID": "posts/sparkify/index.html#business-understanding",
    "href": "posts/sparkify/index.html#business-understanding",
    "title": "Churn prediction using Spark",
    "section": "Business understanding",
    "text": "Business understanding\nChurn prediction is an important classification use case for banks, insurance companies, telcos, cable TV operators, and streaming services such as Netflix, Hulu, Spotify, and Apple Music. Companies that can predict customers who are more likely to cancel the subscription to their service can implement a more effective customer retention strategy.\nCustomer churn costs companies approximately $136 billion per year according to a study done by a leading customer engagement analytics firm 1.\n\nResearch done by Bain & Company shows increasing customer retention rates by just 5% increases profits by 25% to 95% 2.\n\nThe justification for spending resources in churn reduction is based on a study made by Lee Resource Inc. where they show that attracting new customers can cost a company five times more than keeping an existing one! 3."
  },
  {
    "objectID": "posts/sparkify/index.html#data-understanding",
    "href": "posts/sparkify/index.html#data-understanding",
    "title": "Churn prediction using Spark",
    "section": "Data understanding",
    "text": "Data understanding\nThe dataset has a total of 543,705 rows and 18 columns. The schema is the following:\n\n\n\nDataset schema\n\n\nWe have a total of 22 unique entries in the page category.\n\n\n\npage column content\n\n\nI defined a new column called churn that consists of any of Cancellation Confirmation or Submit Downgrade events as a confirmation of a user that has left the service or stop paying for it (free subscription). The distribution of hits per page is:\n\n\n\nDistribution of hits per page\n\n\nWe can see that NextSong page is accessed a lot, which makes sense as it indicates users are listening to songs. Next, is Home followed by three that indicate interaction with the service: Thumbs Up, Add to Playlist and, Add Friend. These three indicate a positive experience with the service. On the other hand, we have Roll Advert, Thumbs Down and, Error as possible indicators of a bad experience for users with the service.\n\nAs we can see from the summary, we are facing a very unbalanced dataset. The ratio of no-churn (0) and churn (1) is 2,516.\nThere is a total of 225,393 female and 302,612 male users with 15,700 users not revealing their gender that I have categorized as U (Unavailable):\n\n\n\nGender column distribution\n\n\nWe have a total of 428,597 paid users and 115,108 users in the free plan/service. As we have stated before we have to make sure that we keep these paid subscribers as much as we can to maximize the revenue (or minimize the loss).\n\n\n\npaid vs free users"
  },
  {
    "objectID": "posts/sparkify/index.html#data-preparation",
    "href": "posts/sparkify/index.html#data-preparation",
    "title": "Churn prediction using Spark",
    "section": "Data preparation",
    "text": "Data preparation\nThe first thing I tackled was to solve the unbalance issue. I used a technique called over-sampling. It is a very basic method where I took advantage of PySpark’s explode dataframe feature to select as many events from the underrepresented class (churn equals 1 in this case) to fill in the difference until I got a balanced data set to work with.\nThere are more advanced methods that I read about it but they are mostly built for Pandas/Numpy dataframes/sets and did not parallelize well in my Spark environment. This definitively needed more time and investigation to find a more robust solution. The most promising method I learned is SMOTE where interested readers can find more details here and here.\nAfter applying this technique I have expanded and balanced the dataframe augmenting it ~50%:\n\n\n\nBalanced data set\n\n\nGoing from a total of 543,705 entries to a total of 1,086,945. This proved very useful as my model’s accuracy improved and I reduced significantly the overfit problems I faced without it. Even though it is not perfect and can be improved further.\nThe userAgent provides some information I build two features that I used: os and browser.\n\n\n\nBrowser count\n\n\nFrom the browser, we can see that 642,801 use Safari and 249,372 use Firefox.\n\n\n\nOS distribution\n\n\nWindows is the more used operating system followed by Macintosh and X11 (Linux). iPhone is the 4th more used and compatible maybe means Android?\nI used the ts (timestamp) column to build more features. From the ts I constructed the day_of_week and hour column.\nUsing these new columns we can see that users tend to listen to more songs towards the end of the day. Users start listening after lunch, peaking at 4–5 PM (during the commute drive?).\n\n\n\nNumber of songs played by the hour of the day\n\n\nUsers listen to songs more during weekdays too. Thursday seems to be the day that stands out but not by much to make conclusions.\n\n\n\nNumber of songs played by day of the week\n\n\nI have constructed the following features:\n\nsaved_settings Count of all ‘Save Settings’ page event grouped by ‘userId’\nnum_songs Count of number of ‘song’ played by users (grouped by ‘userId’)\nthumbs_up Count of all ‘Thumbs Up’ page event grouped by ‘userId’\nnum_advertisement Count of all ‘Roll Advert’ page event grouped by ‘userId’\nthumbs_down Count of all ‘Thumbs Down’ page event grouped by ‘userId’\nplaylist_added Count of all ‘Add to Playlist’ page event grouped by ‘userId’\nfriend_added Count of all ‘Add Friend’ page event grouped by ‘userId’\nerrors_pages Count of all ‘Error’ page event grouped by ‘userId’\nsongs_persession Average songs (by ‘Next Song’ page event) played by users (grouped by ‘userId’) on a given session (‘sessionId’)\n\nI have used StringIndexer, VectorAssembler and, Normalizer from PySpark ML feature’s library. StringIndexer encodes a string column of labels to a column of label indices. VectorAssembler which is a transformer combines a given list of columns/features into a single vector column as required by the ML algorithms. Finally, a Normalizer is a Transformer that transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies the p-norm used for normalization. (p=2 by default.) This normalization can help standardize your input data and improve the behavior of learning algorithms."
  },
  {
    "objectID": "posts/sparkify/index.html#modeling",
    "href": "posts/sparkify/index.html#modeling",
    "title": "Churn prediction using Spark",
    "section": "Modeling",
    "text": "Modeling\nI have split the dataset in training and testing sets with an 80–20 percent split respectively to move to the modeling phase.\nFor modeling, I used Logistic Regression, Random Forest Classifier, GBT Classifier, and Naive Bayes algorithms from the Spark ML library. I measured the best performer, using the F-1 score metric as a parameter for all of them to select the best and fine-tune it.\nF-1 score makes more sense for our churn rate prediction model because we are more interested in the False Negatives and False Positives. The first one because it indicates that we predicted users not leaving that did churn. The second one indicates users we predict leaving that did not leave. With the above, I am not saying that True Negative (predicted users not leaving who leave the service) is not important too!"
  },
  {
    "objectID": "posts/sparkify/index.html#model-selection",
    "href": "posts/sparkify/index.html#model-selection",
    "title": "Churn prediction using Spark",
    "section": "Model selection",
    "text": "Model selection\nI trained Logistic Regression, Random Forest Classifier, GBT Classifier, and Naive Bayes algorithms with the default parameters on the ‘train’ dataset and evaluated it with the ‘test’ dataset.\n\n\n\nFigure 1.1: Model results summary\n\n\nWhere:\n\nF1 - F-1 score\nWP - Weighted Precision\nWR - Weighted Recall"
  },
  {
    "objectID": "posts/sparkify/index.html#best-model-selection",
    "href": "posts/sparkify/index.html#best-model-selection",
    "title": "Churn prediction using Spark",
    "section": "Best model selection",
    "text": "Best model selection\nFrom Figure 1.1 we can see that the best performing model is GBTClassifier. I fine-tuned this model running a Cross-Validation with 5 folds and a parameter grid as follows:\n\n\n\nParameters grid for model fine-tune\n\n\nI evaluated the model with the same metrics as before obtaining the following values:\n\n\n\nFigure 1.2: Best model parameters summary"
  },
  {
    "objectID": "posts/sparkify/index.html#conclusion",
    "href": "posts/sparkify/index.html#conclusion",
    "title": "Churn prediction using Spark",
    "section": "Conclusion",
    "text": "Conclusion\nAs we can see this fine-tuned model provides a 4% increase in all metrics from the previous run and almost a 10% increase compared to the other models!\nOur model predicted 46,435 users leaving who did leave (True Positive) and 58,499 users not leaving who leave the service (True Negative). The model also predicted 13,790 users leaving who did not leave (False Positive) and 4,047 users not leaving who did leave (False Negative).\nOur model though is far from perfect. With a precision of only 77% and a recall of 92% means we predict accurately 2/3 of the churn cases correctly.\nGBTClassifier was the best of all the algorithms I tried in this project and also was the one that took longer to train as it trains one tree after the other. I had issues in the performance of the EMR cluster and had to do some configurations to be successful in training all these models in Spark. For all these details please head to my GitHub repository.\nAs I mentioned before there is plenty of room to improve the over/under-sample of this imbalanced dataset. If time and budget permits a more broad grid search can be performed with more hyperparameters than I used here to improve the model performance but that will require, time and budget as EMR is not a free service."
  }
]